{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Authors: Josh Domeyer & Eda Zhang\n",
    "#Date: 10/25/2018\n",
    "#Project: NLTK 2-Week Project\n",
    "\n",
    "#Description:\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import html5lib\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "import wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Scrapes list of wikipedia \"list of disney theatrical animated features\"\n",
    "# and captures the corresponding summary of each of their pages\n",
    "#\n",
    "\n",
    "disney_movies = wikipedia.page(\"List of Disney theatrical animated features\")\n",
    "table_scrape = BeautifulSoup(disney_movies.html(), 'html.parser')\n",
    "all_tables = table_scrape.findAll(\"table\", \"wikitable\")\n",
    "movies_table = pd.read_html(str(all_tables[1]), header = 0)\n",
    "\n",
    "link_list = []\n",
    "for link in all_tables[1].findAll('a'):\n",
    "    link_list.append(link.get('href'))\n",
    "    \n",
    "link_list = [i.strip('/wiki/').\n",
    "             replace('_', ' ').\n",
    "             replace('%27', '\\'').\n",
    "             replace('%26', '&') for i in link_list if ('wiki' in i) & ('#' not in i) & \n",
    "             ('Productions' not in i) & ('Studios' not in i) & ('Animation' not in i) & \n",
    "             ('Motion Picture' not in i) & ('ImageMovers' not in i) & ('Entertainment' not in i) & \n",
    "             ('Films' not in i) & ('UTV' not in i) & ('C.O.R.E.' not in i)]\n",
    "\n",
    "movies_df = pd.DataFrame(movies_table[0]).iloc[:,0:2]\n",
    "movies_df['WikiLink'] = link_list\n",
    "movies_df = movies_df.rename(index=str, columns={\"Title\": \"Title\", \n",
    "                                     \"Original theatrical release date[rls 1]\": \"Release\", \n",
    "                                     \"WikiLink\": \"WikiLink\", \"Contents\": \"Content\"})\n",
    "\n",
    "movies_df.iloc[6,2] = movies_df.iloc[6,2].replace('Bamb','Bambi')\n",
    "\n",
    "movie_pages = [wikipedia.page(i) for i in movies_df.iloc[:,2]]\n",
    "movies_df['Content'] = [i.content for i in movie_pages]\n",
    "\n",
    "movies_df['Content']\n",
    "\n",
    "movies_df['Content_token']=[nltk.word_tokenize (i) for i in movies_df.loc[:,'Content']]\n",
    "\n",
    "#c = movies_df.apply(lambda column: nltk.word_tokenize(column['Content'], axis=3))\n",
    "# movies_df['content_token'] = [nltk.word_tokenize(i) for i in movies_df.loc('Content')]\n",
    "#df = pd.DataFrame({'Content'})\n",
    "#dataframe['new_column'] = [nltk.sent_tokenize(i) for i in dataframe]\n",
    "# df = pd.DataFrame({'sentences': ['This is a very good site. I will recommend it to others.', 'Can you please give me a call at 9983938428. have issues with the listings.', 'good work! keep it up']})\n",
    "# df['tokenized_sents'] = df.apply(lambda row: nltk.word_tokenize(row['sentences']), axis=1)\n",
    "#movies_df['Content'] = movies_df.apply(lambda row: nltk.sent_tokenize (row['Content']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Remove stopwords\n",
    "#Lemma & stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame({'sentences': ['This is a very good site. I will recommend it to others.', 'Can you please give me a call at 9983938428. have issues with the listings.', 'good work! keep it up']})\n",
    "# df['tokenized_sents'] = df.apply(lambda row: nltk.word_tokenize(row['sentences']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #\n",
    "# # Extra code from my first attempt at characters\n",
    "# #\n",
    "\n",
    "# from html.parser import HTMLParser\n",
    "\n",
    "# class MLStripper(HTMLParser):\n",
    "#     def __init__(self):\n",
    "#         self.reset()\n",
    "#         self.strict = False\n",
    "#         self.convert_charrefs= True\n",
    "#         self.fed = []\n",
    "#     def handle_data(self, d):\n",
    "#         self.fed.append(d)\n",
    "#     def get_data(self):\n",
    "#         return ''.join(self.fed)\n",
    "\n",
    "# def strip_tags(html):\n",
    "#     s = MLStripper()\n",
    "#     s.feed(html)\n",
    "#     return s.get_data()\n",
    "\n",
    "# disney_lines = [p for p in strip_tags(\n",
    "#     wikipedia.page(\"List of Disney characters\").html()).split('\\n') if p]\n",
    "\n",
    "# disney_lines_clean = []\n",
    "# for line in disney_lines:\n",
    "#     disney_lines_clean.append(re.sub(r'([^a-zA-Z ]+?)', '', line))\n",
    "\n",
    "# disney_chars = [re.sub(r'([^a-zA-Z ]+?)', '', i).strip().lower() \n",
    "#                 for i in nltk.sent_tokenize('. '.join(disney_lines_clean)) \n",
    "#                 if (len(i) > 4) & (len(i) < 15) & ('None' not in i) \n",
    "#                 & ('The' not in i) & ('Unknown' not in i)]\n",
    "\n",
    "# #disney_chars = list(set(disney_chars))\n",
    "# #disney_chars = [i for i in disney_chars if wikipedia.search(i, results = 1, suggestion = False)]\n",
    "\n",
    "# #disney_chars.sort()\n",
    "\n",
    "# #with open('./disney_chars.txt', 'w') as f:\n",
    "# #    for item in disney_chars:\n",
    "# #        f.write(\"%s\\n\" % item)\n",
    "\n",
    "# #disney_chars\n",
    "\n",
    "\n",
    "# #Examples of Wikipedia package operations\n",
    "\n",
    "# #print(wikipedia.summary(\"Adolf Hitler\", sentences = 1))\n",
    "# #wikipedia.search(\"Adolf Hitler\", results = 20)\n",
    "\n",
    "# #adolf = wikipedia.page(\"Adolf Hitler\")\n",
    "# #adolf.title\n",
    "# #adolf.url\n",
    "# #adolf.images[0]\n",
    "# #adolf.content #Entire text\n",
    "# #adolf.links[10]\n",
    "\n",
    "# #print(wikipedia.suggest(\"Barack Obama\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
